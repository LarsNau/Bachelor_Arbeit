%========================================================================================
% TU Dortmund, Informatik Lehrstuhl VII
%========================================================================================

\chapter{Punktwolkenverarbeitung zur Bauteilausrichtung}
\label{Punktwolkenverarbeitung zur Bauteilausrichtung}

\section{Ziel der Punktwolkenverarbeitung}
\label{Ziel der Punktwolkenverarbeitung}

Um ein Bauteil korrekt auszurichten, muss eine Fläche des Bauteils auf eine Ausrichtung abgebildet werden. Die Flächen in den Bauteilen wurden in Kapitel \ref{Flächensuche in  Bauteilen} identifiziert und als Punktwolken gespeichert.
Punktwolken wurden im Kapitel \ref{Flächensuche in  Bauteilen} bereits beschrieben. Eine Punktwolke ist eine Menge an Punkten im 3D Raum, die zusammen eine Fläche oder ein Objekt darstellen.
Die Ausrichtung die berechnet wird, wird Z-Invarianz genannt. Die Z-Invarianz ist ein $R^{3}$ Vektor, der die Rotation eines Bauteils beschreibt.
Dieser Vektor sollte im optimalen Fall im 90 Grad Winkel zu dem Normalvektor einer Fläche liegen, damit die Fläche möglichst parallel zu der Achse der Druckerdüse ist. Eine Fläche, die parallel zu der Düse des Druckers ist, 
minimiert den Überhang, den die Fläche haben kann. Der Normalvektor der Fläche ist aber ein Durchschnitt über die Ausrichtung aller Punkte in der Fläche. Wie die Ausrichtung der Punkte bestimmt wird, wird in Abschnitt \ref{Normalvektoren} erklärt.
Wie aus der Z-Invarianz eine Rotationsmatrix erstellt wird, wird in Abschnitt \ref{Konstruktion der Ausrichtung} erklärt.

\smallskip

In diesem Kapitel wird dargestellt, wie man eine Punktwolke, die eine Fläche in einem Bauteil darstellt, zu einer Rotationsmatrix verarbeiten kann. Für die Verarbeitung wird ein Neurales Netz eingesetzt, welches im Folgenden genauer erklärt wird. 
Dieses Neurale Netz wird eine Punktwolke beliebiger Größe zu der Z-Invarianz transformieren. Die Z-Invarianz wird in einem folgenden Abschnitt \ref{Konstruktion der Ausrichtung} erklärt. Aus dieser Ausgabe lässt sich dann eine vollständige Drehung erstellen.

\section{Punktwolken Transformation}
\label{Punktwolken Transformation}

Die Transformation der Punktwolken erfolgt über ein Neurales Netz. Ein Neurales Netz kann Daten verarbeiten, indem es sie durch viele Schichten an künstlichen Neuronen durchleitet. 
Ein künstliches Neuron ist eine Funktion, die als Eingabe andere künstliche Neuronen oder echte Daten nimmt. 
Die Eingabe des Neurons besteht aus den einzelnen Eingaben ($x_{i}$) multipliziert mit den Gewichten für die einzelnen Eingaben ($w_{i}$).

\begin{equation}
    \sum_{ i = 1 }^{ n }{ x_{i} * w_{i} } = o_{j}
\end{equation}

Diese Funktion bildet die gewichteten Eingaben dann auf eine Ausgabe $o_{j}$ mittels Aktivierungsfunktion ab, 
die entweder an ein weiteres Neuron übergeben wird, oder die Ausgabe des Netzes darstellt.
Die Abbildung in der Aktivierungsfunktion kann jede Funktion sein, die einen Wert x auf einen anderen Wert y abbildet, allerdings sind nicht lineare Funktionen wie zum Beispiel: $y = \max(0,x)$ (ReLU) generell besser geeignet, 
da man nur mit dieser Art von Aktivierungsfunktionen nicht lineare Zusammenhänge darstellen kann.

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.25]{bilder/ArtificialNeuronModel_deutsch}
    \caption[Funktionsweise eines künstlichen Neurons]{Funktionsweise eines künstlichen Neurons}
            \label{Funktionsweise eines künstlichen Neurons}
    \quelle\url{https://upload.wikimedia.org/wikipedia/commons/7/7f/ArtificialNeuronModel_deutsch.png}
\end{figure}

Ein typisches Neuron ist in Abbildung \ref{Funktionsweise eines künstlichen Neurons} dargestellt. Durch das Anpassen von Gewichten,$w_{i}$ in der Abbildung \ref{Funktionsweise eines künstlichen Neurons}, kann ein Neuron den Einfluss von Eingabedaten ($x_{i}$) verändern. 
Die optimale Belegung von Gewichten wird von dem Netz über mehrere Versuche hinweg erarbeitet. Dafür muss definiert werden, was das Neurale Netz als korrektes Ergebnis ansieht. Für die Anpassung der Gewichte in jedem Neuron wird ein Datensatz genutzt, 
den das Neurale Netz mehrmals durchläuft. Diese Daten werden Trainingsdaten genannt. Bei jeden Durchlauf der Trainingsdaten
überprüft das Neurale Netz, wie gut seine Ausgaben im Vergleich zu dem vordefinierten korrekten Ergebnis ist. Die Trainingsdaten brauchen für jeden Datenpunkt ein richtiges Ergebnis, damit das Neurale Netz sein Ergebnis mit dem vorher berechnet 
oder festgelegtem Optimum vergleichen kann. In dem Falle ist das richtige Ergebnis der Normalvektor zu der jeweiligen Punktwolke und das Netz nimmt die Standardabweichung von der Orthogonalität zu dem Normalvektor der Fläche als Fehler. 
Umso mehr die Z-Invarianz in einem 90 Grad Winkel zu dem Normalvektor der Fläche steht umso geringer ist der Fehler. Das Netz sieht jede Bewegung des Bauteils ebenfalls als Fehler. Umso größer die Bewegung umso der Fehler. 
Die beiden Fehler werden zusammenaddiert und ergeben dann den Fehler, den das Netz zu verringern versucht, indem es Gewichte anpasst. Der Fehler den die Bewegung mach, wird aber vor der Addition künstlich verringer, 
um die Ausrichtung der Fläche als wichtigsten Aspekt des Netzes zu behalten.

\smallskip

Die Gewichte in den Neuronen werden vor der Anpassung zufällig festgelegt. In jeder Iteration werden die Gewichte an den Neuronen angepasst, falls dies zu einem besseren Ergebnis führt. Dafür wird über Gradienten berechnet, wie sehr jedes Gewicht zu dem Fehler beigetragen hat.
Über viele Iterationen versucht das Netz dadurch den Fehler (engl. Loss) zu minimieren.

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.4]{bilder/Regression Net.jpg}
    \caption[Schematische Darstellung von Regressionsnetz]{Schematische Darstellung von Regressionsnetz}
            \label{Schematische Darstellung von einem beispielhaften Regressionsnetz}
    \quelle\url{https://miro.medium.com/1*LaEgAU-vdsR_pClMcgbikQ.jpeg}
\end{figure}


In vereinfachte Darstellung der Schichten eines Neurales Netzes ist in Abbildung \ref{Schematische Darstellung von einem beispielhaften Regressionsnetz}. Eine Schicht an künstlichen Neuronen ist so definiert, 
dass man die Neuronen der Schicht immer mit der gleichen Anzahl an Kantenübergängen erreicht. Jedes Neuron der ersten verborgenen Schicht kann zum Beispiel mit nur genau einem Kantenübergang erreicht werden.
Für eine vollständig verbundene Schicht muss jedes künstliche Neuron mit jedem künstlichen Neuron der vorherigen und darauffolgenden Schicht verbunden sein. Dabei gibt es drei Arten von Schichten, wie in der Abbildung \ref{Regression auf Merkmalsvektor} zu sehen. Zuerst gib es die Eingabeschicht.
Diese Schicht repräsentiert die Eingaben und ist in Abbildung \ref{Schematische Darstellung von einem beispielhaften Regressionsnetz} rot markiert. Jeder Knoten der Eingabeschicht ist ein Datenpunkt, oder in diesem Fall ein Merkmal. 
Danach gibt es die versteckten Schichten, in Blau markiert. Ihre Aufgabe ist es die Eingabe zu transformieren. 
Sie erlauben es dem Neuralen Netz eine Eingabe über mehrere Schichten zu verarbeiten. Durch kann das Neurale Netz nicht lineare zusammenhänge darstellen. Zuletzt gibt es die Ausgabeschicht. Von dieser Schicht wird die Z-Invarianz abgelesen.

\smallskip

Nicht alle neurale Netze funktionieren über mehrere voll verbundene Schichten, aber die Idee eine Eingabe schichtweise durch das Anpassen von Gewichten zu verarbeiten kann in fast allen gefunden werden.
Das für diese Arbeit genutzte neurale Netz reduziert erst die Eingabe auf Merkmale und nutz dann mehrere voll verbundene Schichten um aus den Merkmalen eine Ausgabe zu generieren. Merkmale sind Darstellungen der Eingabedaten, die das neurale Netz verstehen kann.
Merkmale sind, wenn sie von einem neuralen Netz gefunden wurden, für gewöhnlich extrem abstrakt und ohne viel Arbeit nicht für Menschen verständlich.

\subsection{Merkmalsextraktion aus Punktwolke}
\label{Merkmalsextraktion aus Punktwolke}

Wenn eine Eingabe ohne Umwege auf die Ausgabe abgebildet werden soll, muss ein tiefes Netz eingesetzt werden. Um ein Bauteil automatisch auszurichten, ist es notwendig, dass das Neurale Netz die Eingabedaten selber verarbeitet.
Der Unterschied zwischen einem tiefen und nicht tiefen Neuralen Netz ist es, dass ein tiefes Neurales Netz nicht nur die Abbildung von Merkmalen auf Ausgaben erlernt, sondern auch die Extraktion von Merkmalen aus den Eingabedaten. 
Die Extraktion der Merkmale wird in diesem Abschnitt erklärt.
In der Merkmalsextraktion wird eine Punktwolke auf eine Anzahl an Merkmalen reduziert. Die Merkmale, auf welche die Punkwolke reduziert wird, werden in einem flachen Vektor $(v_{0},v_{1},v_{2},\ldots,v_{n})$ mit $v_{i} \in \mathbb{R}$ der Größe n repräsentiert.
Dieser Vektor wird Merkmalsvektor genannt und ist eine abstrakte Darstellung der Punktwolken. Der Merkmalsvektor ist als Zwischenergebnis wichtig, um die Eingabedaten in für ein Regressionsnetz besser verarbeitbare Form zu bringen. Für Menschen ist der Inhalt eines Merkmalsvektors meistens unverständlich.
Das Regressionsnetz ist der zweite Teil der Struktur des Neurales Netzes und wird in dem folgenden Abschnitt \ref{Regression auf Merkmalsvektor} beschrieben.

\smallskip

Die Struktur der Merkmalsextraktion basiert auf der Arbeit:  \cite{PointNet++}

\smallskip

Die generelle Struktur der Merkmalsextraktion ist eine wiederholte Folge an Reduzierungen auf den konkreten Daten der Punktwolke, gefolgt von einem kleinem voll verbundenen Neuralen Netz, was mit den konkreten Daten dieser Schicht Merkmale über mehrere Schichten hinweg konstruiert.
In dem ersten Schritt in jeder Schicht werden Punkte aus der Punktwolke ausgewählt. Dafür wird ein Algorithmus eingesetzt, der eine Teilmenge der Ausgangspunkte auswählt, um eine Möglichst gute Abdeckung über die Ausgangspunkte zu erreicht (Farthest Point Sampling) \cite{Point}.


\begin{figure}[t]
    \centering
    \includegraphics[scale=0.28]{bilder/Pointnet++.jpg}
    \caption[Schematische Darstellung von Merkmalsextraktionschichten]{Schematische Darstellung von Merkmalsextraktionschichten}
            \label{Schematische Darstellung von Merkmalsextraktionschichten} 
    \quelle\url{https://stanford.edu/~rqi/pointnet2/images/pnpp.jpg}
\end{figure}

Dafür geht der Algorithmus schrittweise vor. In jedem Schritt wählt er den Punkt der Punktwolke aus, der am weitesten von allen bisher ausgewählten Punkten entfernt ist, solange bis die gewünschte Menge an Punkten erreicht ist.
Der Algorithmus sucht nicht nach der optimalen Abdeckung, welche die Distanz von jedem nicht ausgewählten Punkt zu dem nächsten ausgewählten Punkt minimiert, sondern ist nur eine Abschätzung, dieser optimalen Lösung.
Die gefundenen Punkte bilden die ausgewählten Punkte für die aktuelle Schicht. Danach werden für jeden ausgewählten Punkt jeder Punkt innerhalb eines Radius um sich selber verbunden. Diese Punkte sind die Nachbarn des ausgewählten Punktes.
In der Abbildung \ref{Schematische Darstellung von Merkmalsextraktionschichten} wir die Auswahl von Punkten und die Reduktion der ausgewählten Punkte in dem "Sampling and Grouping" Schritt gezeigt. Es ist zu sehen, 
dass um bestimmte Punkte nach diesem Schritt ein Radius gezeichnet ist, der die Nachbarn der ausgewählten Punkte darstellt.
Wenn die Auswahl der Punkte $A$ ist und die gesamte Punktwolke $P$ ist, dann gilt:

\begin{align}
    A \Subset P, (E,V) = G, \\
    \forall \vec{a} \in A , \forall \vec{p} \in P: \left|  \left| \vec{a} - \vec{p}  \right| \right|_{2} \leq r \implies (p,a) \in E
\end{align}

Die ausgewählten Punkte werden als künstliche Neuronen genutzt. Dabei sind die Merkmale, die in den Punkten der Nachbarn gespeichert sind, die Eingabe für das künstliche Neuron. Die Gewichte für jede Eingabe werden erlernt, dafür wird ein Multi Layer Perceptor (MLP) genutzt.
Der MLP lernt, wie die Gewichte für die oben bestimmten Kanten seinen sollen. Dafür nutz der MLP die relative Position von ausgewählten Punkten zu ihren Nachbarn. Ein MLP funktioniert wie ein voll verbundenes Netz, was in Abschnitt \ref{Punktwolken Transformation} erklärt wird.
Ein ausgewählter Punkt verändert seine Merkmale anhand der Summe von Merkmalen der Punkte in seiner Nachbarschaft, die nach ihrer relativen Position mit dem MLP gewichtet werden. Die Anzahl an Punkten verringert sich, aber die Anzahl der Merkmale pro Punkt erhöht sich mit jeder Schicht.

Über mehrere Schichten hinweg, werden Merkmale für die Punkte gelernt, um dann die Punkte zu reduzieren und die Merkmale an die ausgewählten Punkte weiterzugeben. Das generiert eine Menge an Merkmalen für jeden Punk, 
aber für die Weiterverarbeitung wird ein Merkmalsvektor benötigt, der das gesamte Bauteil beschreibt. Im letzten Schritt wird das größte Merkmal in jeder Dimension der Merkmale aller Punkte genommen. So wird ein flacher Merkmalsvektor erstellt.
Mit dem größten Merkmal ist das numerisch Größte gemeint, nicht unbedingt das Wichtigste. 

\subsection{Regression auf Merkmalsvektor}
\label{Regression auf Merkmalsvektor}

Nach der Bestimmung von abstrakten Merkmalen in einem Merkmalsvektor im vorherigen Abschnitt \ref{Merkmalsextraktion aus Punktwolke} muss aus diesem Vektor ein konkretes Ergebnis abgeleitet werden. Dafür wird eine Abbildung von dem Vektor auf die Ergebnisse erlernt. Dieses Vorgehen wird Regression genannt. 
In diesem Fall wird auf die Z-Invarianz regressiert. Der Merkmalsvektor wird über mehrere Schichten auf eine Ausgabe abgeleitet.

\smallskip

Eine Schicht funktioniert so, dass jedes Neuron die gewichtete Summe von allen vorhergehenden Neuronen erhält und diese Summe an eine Aktivierungsfunktion übergibt. Dabei werden Methoden wie Normalisierung und zufälliges zurücksetzten von Eingaben verwendet, um
die Transformation verlässlicher zu machen. Es sind voll verbundene Schichten, die in Abschnitt \ref{Punktwolken Transformation} beschrieben wurden.
In jeder Schicht werden Maßnahmen wie Normalisierung und zufälliges zurücksetzen eingesetzt, um die Regression stabiler zu machen.

Die Normalisierung stellt sicher, dass es keine zu großen Änderungen innerhalb einer Schicht vorkommen, da alle Werte relativ zu dem Durchschnitt normalisiert werden. Bei einer großen Änderung innerhalb einer Schicht, zwingt es darauffolgende Schichten ebenfalls Änderungen vorzunehmen. 
Die konstante Anpassung zu Änderungen in den vorherigen Schichten macht das Lernen instabil.

Das Zufällige zurücksetzen von Eingaben verhinder eine zu große Abhängigkeit von nur einem Neuron oder einem Pfad. Dafür werden mit einer vorbestimmten Wahrscheinlichkeit gewichtete Summen auf null gesetzt, sodass sie keinen Einfluss auf die nächste Schicht haben.

\smallskip

In jeder Schicht des Regressionsnetzes wird dieselbe Aktivierungsfunktion genutzt. In allen Schichten außer der letzten ist das $f(x) = \max(0,x)$ die als Rectifier oder ReLU bekannt ist. 
Es ist eine der bekanntesten Aktivierungsfunktionen, unter anderem weil sie einfach, aber effektiv ist. Für die letzte Aktivierung wird dagegen 

\begin{equation}
    f(x) = \frac{\exp(x) - \exp(x)}{\exp(x) + \exp(x)}
\end{equation}

genutzt. Diese Funktion wird tanh genannt.
Sie hat den Vorteil, dass ihre Ausgabe immer zwischen -1 und 1 liegt, was genau das Intervall an Werten ist, die in einer Rotationsmatrix zugelassen sind. Die Ausgabe muss also nicht im korrigiert werden, um für eine Rotationsmatrix zulässig zu sein.

\smallskip

Das Ergebnis der Regression ist die Z-Invarianz.
Die Z-Invarianz ist aber noch keine Rotationsmatrix und muss daher zu einer gemacht werden, um ein Bauteil drehen zu können, 
was in Abschnitt \ref{Konstruktion der Ausrichtung} beschrieben wird.


\section{Konstruktion der Ausrichtung}
\label{Konstruktion der Ausrichtung} % wie wird aus z Invarianz und Z drehung eine Rotation (und warum)

Die, in Abschnitt \ref{Regression auf Merkmalsvektor} berechnete, Z-Invarianz ist nicht genug, um eine Drehung darzustellen, um ein Bauteil drehen zu können ist eine Rotationsmatrix notwendig. 
Rotationsmatrizen sind, für den 3d Raum, 3 x 3 Matrizen, die durch Matrixmultiplikation eine Punktwolke drehen können.
Eine Punktwolke bestehend aus mehreren Punkten  $(x_{i},y_{i},z_{i})$ kann wie folgt durch Matrixmultiplikation mit einer Rotationsmatrix $R$ gedreht werden.

\begin{equation}
    \begin{bmatrix}  
    x_1 & y_1 & z_1 \\  
    x_2 & y_2 & z_2 \\  
    \vdots & \vdots & \vdots \\  
    x_n & y_n & z_n \\  
    \end{bmatrix}
    \begin{bmatrix}  
    R_{11} & R_{12} & R_{13} \\  
    R_{21} & R_{22} & R_{23} \\  
    R_{31} & R_{32} & R_{33}
    \end{bmatrix}  
    =
    \begin{bmatrix}  
    x_1' & y_1' & z_1' \\  
    x_2' & y_2' & z_2' \\  
    \vdots & \vdots & \vdots \\  
    x_n' & y_n' & z_n' \\  
    \end{bmatrix} 
\end{equation}

Dabei ist die Norm von jeder Zeile gleich Eins und jede Zeile ist, wenn man sie als Vektor betrachtet orthogonal zu den anderen Zeilen. Dasselbe gilt für Spalten.

\begin{align}
    \forall i \in \{1 , 2, 3 \}: \left|  \left| \begin{bmatrix} R_{1i} \\ R_{2i} \\ R_{3i} \end{bmatrix} \right| \right|_{2} = 1 \\
    \forall i \in \{1 , 2, 3 \}: \left|  \left| \begin{bmatrix} R_{i1} \\ R_{i2} \\ R_{i3} \end{bmatrix} \right| \right|_{2} = 1
\end{align}

Um aus dem 3d Vektor, der die Z-Invarianz darstellt, eine Rotationsmatrix zu erstellen, kann die Z-Invarianz selber als Zeile der Rotationsmatrix genommen werden. Die restlichen Zeilen müssen dann aus der ersten Zeile abgeleitet werden.
Die Zeilen einer Rotationsmatrix müssen Orthogonal zueinander sein, daher muss für die anderen Zeilen Vektoren gefunden werden die Orthogonal zu der Z-Invarianz sind. Wenn zwei Vektoren gegeben sind, kann man aus ihnen eine Ebene bilden 
eine Normale dieser Ebene ist Orthogonal zu beiden Vektoren. Dadurch lassen sich orthogonale Vektoren zu der Z-Invarianz finden.



    \tdplotsetmaincoords{70}{110}
    \begin{figure}[t]
        \begin{tikzpicture}[scale=4,tdplot_main_coords]
            \draw[thick,->] (0,0,0) -- (1,0,0) node[anchor=north east]{$x$};
            \def\x{.5}
            \filldraw[
                draw=red,%
                fill=red!20,%
            ]          (0,0,0)
                    -- (\x,{sqrt(3)*\x},0)
                    -- (\x,{sqrt(3)*\x},1)
                    -- (0,0,1)
                    -- cycle;
            \draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
            \draw[blue,very thick, ->] (0,0,0) -- ({1.2*\x},{sqrt(3)*1.2*\x},0) node[below] {$Z-Invarianz$};
            \draw[blue,very thick,->] (0.3,0.51,0.5) -- ({sqrt(3)*0.6*\x + 0.3},{-0.6*\x + 0.51},0.5) ;
            \draw[blue,very thick,->] (0,0,0) -- ({sqrt(3)*0.6*\x},{-0.6*\x},0) node[anchor=north]{$n_1$};
            \draw[thick,dotted] ({sqrt(3)*0.6*\x + 0.3},{-0.6*\x + 0.51},0.5) -- ({sqrt(3)*0.6*\x},{-0.6*\x},0);
            \draw[thick,dotted] (0,0,0) -- (0.3,0.51,0.5);
            \draw[blue,very thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z / Hilfsvektor$};

        \end{tikzpicture}
        \caption[Berechnung von $n_1$]{Berechnung von $n_1$}
            \label{n1}
    \end{figure}

In der Abbildung \ref{n1} wird die Berechnung von der Normale-1 ($n_1$) dargestellt. Die aufgespannte Ebene ist rot gefärbt.
Da die Z-Invarianz aber nur ein Vektor ist, muss für die Bestimmung anderer Zeilen im ersten Schritt ein Hilfsvektor genommen werden. Dafür bietet sich die z-Achse an, da die Rotationen relativ zu der z-Achse bewertet werden.
Zwischen der Z-Invarianz und der z-Achse wird eine Ebene aufgespannt. Die Normale der Ebene ist eine Zeile in der Rotationsmatrix.

\smallskip

Als Hilfsvektor wird immer die normierte z-Achse genommen, daher besteht die Möglichkeit, dass die Z-Invarianz sehr ähnlich zu der z-Achse ist. Das macht die Berechnung der Normalen mathematisch instabil, da durch die extreme Ähnlichkeit jetzt der Nullvektor als Ergebnis kommen könnte. 
Um dieses Problem zu beheben, wird überprüft, ob der Hilfsvektor zu ähnlich zu der Z-Invarianz ist. Wenn sie zu ähnlich sind, wird der Hilfsvektor durch einen anderen Vektor ersetzt. Einen anderen Hilfsvektor zu nehmen beeinflusst die Rotation, aber diese Ausnahme ist sehr selten, verschlechtert sie das durchschnittliche Ergebnis nicht sehr.
Als Alternative zu der z-Achse bieten sich die x und y-Achsen an, da sie beide 90 Grad von der z-Achse entfernt sind und dadurch, die Struktur der Rotation erhalten bleibt.


    
    \tdplotsetmaincoords{70}{110}
    \begin{figure}[t]
        \centering
        \begin{tikzpicture}[scale=4,tdplot_main_coords]
            \def\x{.5}
            \filldraw[
                draw=red,%
                fill=red!20,%
            ]          (0,0,0)
                    -- (\x,{sqrt(3)*\x},0)
                    -- ({\x + sqrt(3)*\x},{sqrt(3)*\x + -1*\x},0)
                    -- ({sqrt(3)*\x},{-1*\x},0)
                    -- cycle;
            \draw[thick,->] (0,0,0) -- (1.5,0,0) node[anchor=north east]{$x$};
            \draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
            \draw[thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};
            \draw[blue,very thick, ->] (0,0,0) -- ({1.2*\x},{sqrt(3)*1.2*\x},0) node[below] {$Z-Invarianz$};
            \draw[blue,very thick,->] (0,0,0) -- ({sqrt(3)*1.2*\x},{-1.2*\x},0) node[anchor=north]{$n_1$};
            \draw[blue,very thick,->] (0,0,0) -- (0,0,0.5) node[anchor=east]{$n_2$};
            \draw[blue,very thick,->] (0.8,0.2,0) -- (0.8,0.2,0.5) ;
            \draw[thick,dotted] (0,0,0.5) -- (0.8,0.2,0.5);
            \draw[thick,dotted] (0,0,0) -- (0.8,0.2,0);
        \end{tikzpicture}
        \caption[Berechnung von $n_2$]{Berechnung von $n_2$}
            \label{n2}
    \end{figure}

In der Abbildung \ref{n2} wird die Berechnung von der Normale-2 ($n_2$) dargestellt. Die aufgespannte Ebene zwischen $n_1$ und der Z-Invarianz ist rot gefärbt. Senkrecht zu der roten Ebene ist der $n_2$ zu sehen. $n_2$ wird berechnet, indem der Vektor, 
der sowohl zu $n_1$, als auch zu der Z-Invarianz senkrecht steht. Das kann über das Kreuzprodukt erreicht werden.
Durch das Berechnen einer anderen Zeile außer der Z-Invarianz ist ein Hilfsvektor nicht mehr vonnöten. Die berechnete Normale $n_2$ bildet die letzte Zeile, die für eine Rotationsmatrix vonnöten ist. Sie ist orthogonal zu den anderen Zeilen, da sei eine Normale in der Ebene der anderen Zeilen ist.
Normale-1 ist ebenfalls orthogonal zu der Normale-2, da es in der Ebene ist, dessen normale Normale-2 ist. Sie ist ebenfalls orthogonal zu der Z-Invarianz, da Normale-1 aus Ebene, die die Z-Invarianz beinhaltet gebildet wurde. 
Orthogonalität ist symmetrisch, daher ist auch die Z-Invarianz orthogonal zu Normale-1 und Normale-2. Das erfüllt die oben genannte Bedingung, dass alle Zeilen orthogonal zueinander seien müssen.



    \begin{figure}[t]
        \centering
        \subfigure[]
            {\includegraphics[scale=0.25]{bilder/Rotation_V1}\label{Rotation_V1}
        }
        \subfigure[]
            {\includegraphics[scale=0.25]{bilder/Rotation_V2}\label{Rotation_V2}
        }
        \caption[Vertauschen von $n_1$ und $n_2$ in der Rotationsmatrix]{Vertauschen von $n_1$ und $n_2$ in der Rotationsmatrix}
            \label{Vertauschen}
    \end{figure}

In der Abbildung \ref{Vertauschen} ist der Unterschied, den das Vertauschen von $n_1$ und $n_2$ machen kann dargestellt.
Die Z-Invarianz wird als unterste Zeile genommen, die beiden berechneten Normalen können eine beliebige andere Zeile nehmen. Die Wahl der Zeilen verändert nur die Rotation des Bauteils um die z-Achse herum, wie man in der Abbildung \ref{Vertauschen} sehen kann. 
Solange also die Zeilen immer gleich angeordnet werden, wird die Gradabweichung von der z-Achse gleich bleiben.

\smallskip

Nachdem die Rotationsmatrix erstellt wurde muss zuletzt überprüft werden, ob sie eine gültige Rotation ist. Im 5d-Druck kann sich die Platform nicht völlig frei bewegen, sie kann eine maximale Gradabweichung vom Ursprung von 90 Grad unterstützen, also insgesamt jeweils 180 Grad entlang zwei Achsen.
Daher wird überprüft, ob die Rotationsmatrix das Bauteil über diese Grenzen hinaus drehen würde. Falls, dem so ist, muss die Rotationsmatrix gespiegelt werden.



    \begin{equation}
        \begin{bmatrix}  
        1 & 0 & 0 \\  
        0 & 1 & 0 \\  
        0 & 0 & -1
        \end{bmatrix}
        \label{Spiegelmatrix}
    \end{equation}

    \begin{figure}[t]
        \centering
            \subfigure[ohne Spiegelmatrix]
                {
                \begin{tikzpicture}[scale=2.5,tdplot_main_coords]
                \def\x{.5}
                \draw[thick,->] (0,0,0) -- (1.5,0,0) node[anchor=north east]{$x$};
                \draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
                \draw[thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};

                \draw[blue,thick,->] (0,0,0) -- (0.6841, -0.6408, -0.3483) node[anchor=south]{};
                \draw[blue,thick,->] (0,0,0) -- (0.0000, -0.4776,  0.8786) node[anchor=south]{};
                \draw[blue,thick,->] (0,0,0) -- (0.7294,  0.6011,  0.3267) node[anchor=south]{};
            \end{tikzpicture}
            }
            \subfigure[mit Spiegelmatrix]
                {
                \begin{tikzpicture}[scale=2.5,tdplot_main_coords]
                \def\x{.5}
                \draw[thick,->] (0,0,0) -- (1.5,0,0) node[anchor=north east]{$x$};
                \draw[thick,->] (0,0,0) -- (0,1,0) node[anchor=north west]{$y$};
                \draw[thick,->] (0,0,0) -- (0,0,1) node[anchor=south]{$z$};

                \draw[blue,thick,->] (0,0,0) -- (0.6841, -0.6408, -0.3483) node[anchor=south]{};
                \draw[blue,thick,->] (0,0,0) -- (0.0000, 0.4776,  -0.8786) node[anchor=south]{};
                \draw[blue,thick,->] (0,0,0) -- (0.7294,  0.6011,  0.3267) node[anchor=south]{};
            \end{tikzpicture}
            }
            \caption[Veränderung der Rotationmatrix durch die Spiegelmatrix]{Veränderung der Rotationmatrix durch die Spiegelmatrix}
            \label{Veränderung der Rotationmatrix durch die Spiegelmatrix}
    \end{figure}

In der Matrix \ref{Spiegelmatrix} wird die Spiegelmatrix gezeigt. In der Abbildung \ref{Veränderung der Rotationmatrix durch die Spiegelmatrix} wird die Veränderungen, die sie auf eine Rotationsmatrix wirkt dargestellt.
Durch die Matrixmultiplikation mit der Spiegelmatrix, aus \ref{Veränderung der Rotationmatrix durch die Spiegelmatrix}, wird eine Rotationsmatrix in der XY-Ebene gespiegelt. Das hat den Effekt, dass sich das Bauteil um 180 Grad dreht. Da eine Spiegelung nötig war, galt vor der Spiegelung, dass der Betrag der Rotation mehr als 90 Grad ist. 
Die Rotation ist also zwischen 90 und 180 Grad. Sie kann nicht höher sein, da eine sinnvolle Rotation nur von -180 bis 180 Grad gehen kann und so einen ganzen Kreis bildet. Wenn man die Rotation spiegelt, wird sie um 180 Grad gedreht.

\smallskip

Wenn sie vor der Spiegelung nicht im 180 Grad Halbkreis der erlaubten Drehungen war, dann ist sie es danach. In dieser Arbeite wird von einem 90 Grad Maximum in der Drehung der Plattform in beide Dimensionen ausgegangen. In der Realität ist es möglich, 
dass ein Drucker zu weniger in der Lage ist, aber 90 Grad ist die höchste Schwenkung der Plattform, bei der die Platform nicht die Düse, des Druckers blockieren kann. Eine Platform die weniger als 90 Grad schwenken kann viele der 
berechneten Ausrichtungen nicht umsetzen. Dieses Szenario wird in dieser Arbeit nicht betrachtet, da nur bewertet werden soll wie Praktikabel der Ansatz ist, Hardwareeinschränkungen sind dafür zuerst zu vernachlässigen.

